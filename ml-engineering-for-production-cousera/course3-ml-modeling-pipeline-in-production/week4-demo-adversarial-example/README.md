# Demo: Adversarial Example
In this demo (`adversarial_fgsm.ipynb`), we follow the process of creating adversarial examples using the Fast Gradient Signed Method (FGSM) attack as described in [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) by Goodfellow et al. This was one of the first and most popular attacks to fool a neural network.

## Requirements
`tensorflow==2.9.2`  
